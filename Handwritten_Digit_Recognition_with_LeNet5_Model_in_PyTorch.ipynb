{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanp031/Handwritten-Digit-Recognition-with-LeNet5-Model-in-PyTorch/blob/main/Handwritten_Digit_Recognition_with_LeNet5_Model_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPMR1q4hKdF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzjNRLX1jG-j",
        "outputId": "6673cd6b-5f08-479c-c7d5-1f915f10754b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "unpack requires a buffer of 4 bytes",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-131d684c2fba>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the images and labels from the IDX files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't10k-images.idx3-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't10k-labels.idx1-ubyte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-131d684c2fba>\u001b[0m in \u001b[0;36mread_idx\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Read the magic number (4 bytes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m  \u001b[0;31m# Extract the number of dimensions from the magic number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Read dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: unpack requires a buffer of 4 bytes"
          ]
        }
      ],
      "source": [
        "# Import the struct module to interpret bytes as packed binary data\n",
        "import struct\n",
        "# Import the numpy library for numerical operations\n",
        "import numpy as np\n",
        "# Import the matplotlib library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def read_idx(filename):\n",
        "    \"\"\"\n",
        "    Reads an IDX file and returns the data as a NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "    filename (str): The path to the IDX file.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: The data contained in the IDX file as a NumPy array.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        magic_number = struct.unpack('>I', f.read(4))[0]  # Read the magic number (4 bytes)\n",
        "        dims = magic_number & 0xFF  # Extract the number of dimensions from the magic number\n",
        "        shape = tuple(struct.unpack('>I', f.read(4))[0] for _ in range(dims))  # Read dimensions\n",
        "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)  # Read the data\n",
        "\n",
        "# Load the images and labels from the IDX files\n",
        "images = read_idx('t10k-images.idx3-ubyte')\n",
        "labels = read_idx('t10k-labels.idx1-ubyte')\n",
        "\n",
        "# Display an example image\n",
        "plt.imshow(images[0], cmap='gray')\n",
        "plt.title(f'Label: {labels[0]}')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LRn4YJ2mvLW"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the images array\n",
        "print(f'Images shape: {images.shape}')\n",
        "# Images shape: (num_images, height, width)\n",
        "# This prints the dimensions of the images array. The shape is a tuple where:\n",
        "# - The first element is the number of images.\n",
        "# - The second element is the height of each image.\n",
        "# - The third element is the width of each image.\n",
        "\n",
        "# Print the shape of the labels array\n",
        "print(f'Labels shape: {labels.shape}')\n",
        "# Labels shape: (num_labels,)\n",
        "# This prints the dimensions of the labels array. The shape is a tuple where:\n",
        "# - The single element is the number of labels.\n",
        "# Each label corresponds to an image.\n",
        "\n",
        "# Print the number of images\n",
        "print(f'Number of images: {len(images)}')\n",
        "# Number of images: num_images\n",
        "# This prints the number of images by getting the length of the images array.\n",
        "# It should match the first element of the images shape.\n",
        "\n",
        "# Print the number of labels\n",
        "print(f'Number of labels: {len(labels)}')\n",
        "# Number of labels: num_labels\n",
        "# This prints the number of labels by getting the length of the labels array.\n",
        "# It should match the number of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmipVEAQwSMR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you've loaded your data using the provided functions\n",
        "images = read_idx('t10k-images.idx3-ubyte')\n",
        "labels = read_idx('t10k-labels.idx1-ubyte')\n",
        "\n",
        "# Define LeNet input shape (assuming grayscale images)\n",
        "leNet_input_shape = (1, 28, 28)\n",
        "\n",
        "# Reshape images to match LeNet's input shape\n",
        "reshaped_images = images.reshape(-1, *leNet_input_shape)  # Reshape with -1 for batch size\n",
        "\n",
        "print(\"Original images shape:\", images.shape)\n",
        "print(\"Reshaped images shape:\", reshaped_images.shape)\n",
        "\n",
        "# Convert data type to float for PyTorch (optional, might be needed depending on your model)\n",
        "reshaped_images = reshaped_images.astype(np.float32) / 255.0  # Normalize pixel values\n",
        "\n",
        "print(\"Data type after reshaping:\", reshaped_images.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI3dnesP7tzl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the datasets with the defined transformations\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define a custom collate function for one-hot encoding\n",
        "def one_hot_collate(batch, num_classes=10):\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "    one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
        "    return images, one_hot_labels\n",
        "\n",
        "# Use the custom collate function in the DataLoader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=lambda x: one_hot_collate(x))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, collate_fn=lambda x: one_hot_collate(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vod6KeF8WDj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to show a batch of images and labels\n",
        "def show_batch(batch):\n",
        "    images, labels = batch\n",
        "    batch_size = len(images)\n",
        "    nrows = int(np.sqrt(batch_size))\n",
        "    ncols = (batch_size // nrows) + (batch_size % nrows > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < batch_size:\n",
        "            ax.imshow(images[i][0], cmap='gray')\n",
        "            ax.set_title(f'Label: {labels[i].argmax().item()}', fontsize=10, pad=10)\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display a batch of images and their one-hot encoded labels\n",
        "dataiter = iter(trainloader)\n",
        "batch = next(dataiter)\n",
        "show_batch(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQHqU4PAJ9pQ"
      },
      "outputs": [],
      "source": [
        "# Display a sample of one-hot encoded labels\n",
        "def display_sample_labels(loader, num_samples=5):\n",
        "    dataiter = iter(loader)\n",
        "    images, one_hot_labels = next(dataiter)\n",
        "    print(\"Sample One-Hot Encoded Labels:\")\n",
        "    for i in range(num_samples):\n",
        "        print(f\"Label {i}: {one_hot_labels[i]}\")\n",
        "\n",
        "# Display sample one-hot encoded labels from the trainloader\n",
        "display_sample_labels(trainloader, num_samples=5)\n",
        "# Count the number of images per class and visualize the distribution\n",
        "def visualize_class_distribution(loader):\n",
        "    class_counts = torch.zeros(10)\n",
        "    for images, one_hot_labels in loader:\n",
        "        labels = one_hot_labels.argmax(dim=1)\n",
        "        for label in labels:\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    # Plot the distribution\n",
        "    classes = [str(i) for i in range(10)]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(classes, class_counts)\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of images')\n",
        "    plt.title('Distribution of Images per Class')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the distribution of images per class in the trainloader\n",
        "visualize_class_distribution(trainloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6rsCbACS3ZE"
      },
      "outputs": [],
      "source": [
        "# Function to show a batch of images with their dimensions and labels\n",
        "def show_batch(batch):\n",
        "    images, one_hot_labels = batch\n",
        "    labels = one_hot_labels.argmax(dim=1)  # Convert one-hot labels to class indices\n",
        "    batch_size = len(images)\n",
        "    nrows = int(np.sqrt(batch_size))\n",
        "    ncols = (batch_size // nrows) + (batch_size % nrows > 0)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 15))  # Increased figure size\n",
        "    fig.subplots_adjust(hspace=0.5, wspace=0.5)  # Added more space between subplots\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < batch_size:\n",
        "            image = images[i][0]\n",
        "            ax.imshow(image, cmap='gray')\n",
        "            ax.set_title(f'Label: {labels[i].item()}', fontsize=10, pad=10)\n",
        "            # Set custom ticks to show dimensions\n",
        "            ax.set_xticks([0, image.shape[1]//2, image.shape[1]])\n",
        "            ax.set_xticklabels([0, image.shape[1]//2, image.shape[1]])\n",
        "            ax.set_yticks([0, image.shape[0]//2, image.shape[0]])\n",
        "            ax.set_yticklabels([0, image.shape[0]//2, image.shape[0]])\n",
        "        ax.axis('on')\n",
        "    plt.show()\n",
        "\n",
        "# Load a batch and display it\n",
        "dataiter = iter(trainloader)\n",
        "batch = next(dataiter)\n",
        "show_batch(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36rXPBaRUEd2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define the sizes of train, validation, and test sets\n",
        "train_size = int(0.7 * len(images))\n",
        "\n",
        "test_size = len(images) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset,  test_dataset = random_split(images, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHY_fJhxb-b6"
      },
      "outputs": [],
      "source": [
        "#print sizes of train, validation, and test sets\n",
        "print(f\"Number of samples in the training set: {len(train_dataset)}\")\n",
        "\n",
        "print(f\"Number of samples in the test set: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBH9T_c1e_QA"
      },
      "outputs": [],
      "source": [
        "# Sizes of train and test sets\n",
        "sizes = [len(train_dataset), len(test_dataset)]\n",
        "labels = ['Train', 'Test']\n",
        "\n",
        "# Plotting the bar chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(labels, sizes, color=['blue', 'red'])\n",
        "plt.xlabel('Dataset Split')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Distribution of Dataset Split')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRKo38FIvYRr"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def read_idx(filename):\n",
        "    \"\"\"\n",
        "    Reads an IDX file and returns the data as a NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "    filename (str): The path to the IDX file.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: The data contained in the IDX file as a NumPy array.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        # Read the magic number (first 4 bytes)\n",
        "        magic = struct.unpack('>I', f.read(4))[0]\n",
        "        # Read the number of dimensions (next byte)\n",
        "        dims = magic & 0xFF\n",
        "        # Read the dimensions (next dims * 4 bytes)\n",
        "        shape = tuple(struct.unpack('>I', f.read(4))[0] for _ in range(dims))\n",
        "        # Read the rest of the file content and convert it into a NumPy array\n",
        "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
        "\n",
        "# Load the images and labels from the IDX files\n",
        "images = read_idx('t10k-images.idx3-ubyte')\n",
        "labels = read_idx('t10k-labels.idx1-ubyte')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBtCLEAstb9n"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer (Adam optimizer with a learning rate of 0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJrHnFSft9YF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Number of epochs to train\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(trainloader, 0):\n",
        "        # Move inputs and labels to GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.argmax(dim=1))  # Use argmax to get class indices from one-hot encoded labels\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f'[Epoch {epoch + 1}, Mini-batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNT9L-UDuC3D"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'lenet5.pth')\n",
        "print('Model saved as lenet5.pth')\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        # Move inputs and labels to GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n",
        "\n",
        "# Ensure the model is back to training mode if needed\n",
        "model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73BRECxO5-aL"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1yqcPAZxv-B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, classes):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        # Move inputs and labels to GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "plot_confusion_matrix(cm, classes=[str(i) for i in range(10)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qKSeuTSBh6y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the datasets with the defined transformations\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define a custom collate function for one-hot encoding\n",
        "def one_hot_collate(batch, num_classes=10):\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "    one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
        "    return images, one_hot_labels\n",
        "\n",
        "# Use the custom collate function in the DataLoader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=lambda x: one_hot_collate(x))\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, collate_fn=lambda x: one_hot_collate(x))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd1x1x0xx1vZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Convert DataLoader to NumPy arrays\n",
        "def dataloader_to_numpy(dataloader):\n",
        "    images_list, labels_list = [], []\n",
        "    for images, labels in dataloader:\n",
        "        images_list.append(images)\n",
        "        labels_list.append(labels)\n",
        "    return np.concatenate(images_list), np.concatenate(labels_list)\n",
        "\n",
        "train_images, train_labels = dataloader_to_numpy(trainloader)\n",
        "test_images, test_labels = dataloader_to_numpy(testloader)\n",
        "\n",
        "# Reshape images to add the channel dimension\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Define the CNN model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Increased dropout for regularization\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=20, batch_size=64, validation_split=0.2, callbacks=[lr_callback])\n",
        "\n",
        "# Save the trained model\n",
        "model.save('cnn_model.h5')\n",
        "print('Model saved as cnn_model.h5')\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.2%}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xs0895hklUl"
      },
      "outputs": [],
      "source": [
        "pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrPbm3kTkudc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 28 * 28  # MNIST image size is 28x28\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input images\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_loss = 0  # Track total loss for the epoch\n",
        "    with tqdm(total=total_step, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({'loss': epoch_loss / (i + 1), 'accuracy': correct / total})\n",
        "            pbar.update()\n",
        "\n",
        "    train_accuracy = correct / total\n",
        "    train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "\n",
        "    print ('Epoch [{}/{}], Train Accuracy: {:.4f}, Val Accuracy: {:.4f}, Loss: {:.4f}'\n",
        "           .format(epoch+1, num_epochs, train_accuracy, val_accuracy, epoch_loss / len(train_loader)))\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mlp_model.pth')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = correct / total\n",
        "    print('Test accuracy: {:.2%}'.format(test_accuracy))\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(range(1, num_epochs+1), train_accuracy_history, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), val_accuracy_history, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5QqAqnRzF5l"
      },
      "outputs": [],
      "source": [
        "# Define the model again\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "model.load_state_dict(torch.load('mlp_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to predict the class of a single image\n",
        "def predict_single_image(image):\n",
        "    image = image.to(device)\n",
        "    image = image.view(1, -1)  # Flatten the image\n",
        "    outputs = model(image)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Test the loaded model with a sample from the test set\n",
        "sample_image, sample_label = test_dataset[0]  # Get the first sample from the test dataset\n",
        "sample_image = sample_image.view(1, 28, 28)  # Reshape for visualization\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
        "plt.title(f'True Label: {sample_label}')\n",
        "plt.show()\n",
        "\n",
        "# Predict the label\n",
        "sample_image = sample_image.view(1, -1)  # Flatten the image\n",
        "sample_image = sample_image.to(device)\n",
        "predicted_label = predict_single_image(sample_image)\n",
        "print(f'Predicted Label: {predicted_label}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltj1mgznt7qU"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit localtunnel tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivEN79-QuBzM"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "\n",
        "# Define the MLP model architecture\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Load the MLP model\n",
        "input_size = 28 * 28\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "mlp_model = MLP(input_size, hidden_size, num_classes)\n",
        "mlp_model.load_state_dict(torch.load('mlp_model.pth', map_location=torch.device('cpu')))\n",
        "mlp_model.eval()\n",
        "\n",
        "# Load the CNN model\n",
        "cnn_model = load_model('cnn_model.h5')\n",
        "\n",
        "# Define the transformation for incoming images for MLP\n",
        "mlp_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def predict_image(image):\n",
        "    pil_image = Image.open(image).convert('L')\n",
        "\n",
        "    # MLP prediction\n",
        "    mlp_image = mlp_transform(pil_image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        mlp_outputs = mlp_model(mlp_image)\n",
        "        _, mlp_predicted = torch.max(mlp_outputs.data, 1)\n",
        "        mlp_result = mlp_predicted.item()\n",
        "\n",
        "    # CNN prediction\n",
        "    cnn_image = np.array(pil_image.resize((28, 28)))\n",
        "    cnn_image = np.expand_dims(cnn_image, axis=-1)\n",
        "    cnn_image = np.expand_dims(cnn_image, axis=0)\n",
        "    cnn_prediction = cnn_model.predict(cnn_image)\n",
        "    cnn_result = np.argmax(cnn_prediction)\n",
        "\n",
        "    return mlp_result, cnn_result\n",
        "\n",
        "st.title(\"Digit Prediction with MLP and CNN\")\n",
        "st.write(\"Upload an image to get predictions from both MLP and CNN models.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=\"png\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    mlp_result, cnn_result = predict_image(uploaded_file)\n",
        "    st.image(uploaded_file, caption='Uploaded Image', use_column_width=True)\n",
        "    st.write(f\"MLP Prediction: {mlp_result}\")\n",
        "    st.write(f\"CNN Prediction: {cnn_result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBh5a3vTuG-a"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9czrJ3aqsO2y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4dcded6-2276-4e35-b3c6-633ae4ff9136"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 600/600 [00:12<00:00, 49.19batch/s, loss=0.292, accuracy=0.919]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Train Accuracy: 0.9195, Val Accuracy: 0.9539, Loss: 0.2918\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 600/600 [00:09<00:00, 61.96batch/s, loss=0.118, accuracy=0.965]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20], Train Accuracy: 0.9652, Val Accuracy: 0.9680, Loss: 0.1184\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 600/600 [00:09<00:00, 62.59batch/s, loss=0.0764, accuracy=0.977]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20], Train Accuracy: 0.9772, Val Accuracy: 0.9748, Loss: 0.0764\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 600/600 [00:09<00:00, 62.86batch/s, loss=0.0552, accuracy=0.983]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20], Train Accuracy: 0.9831, Val Accuracy: 0.9785, Loss: 0.0552\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 600/600 [00:10<00:00, 59.27batch/s, loss=0.0407, accuracy=0.988]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20], Train Accuracy: 0.9879, Val Accuracy: 0.9799, Loss: 0.0407\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 600/600 [00:09<00:00, 60.06batch/s, loss=0.0307, accuracy=0.991]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20], Train Accuracy: 0.9906, Val Accuracy: 0.9809, Loss: 0.0307\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 600/600 [00:10<00:00, 59.89batch/s, loss=0.023, accuracy=0.994]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20], Train Accuracy: 0.9936, Val Accuracy: 0.9806, Loss: 0.0230\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 600/600 [00:10<00:00, 59.90batch/s, loss=0.017, accuracy=0.995]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20], Train Accuracy: 0.9953, Val Accuracy: 0.9805, Loss: 0.0170\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 600/600 [00:10<00:00, 59.80batch/s, loss=0.0139, accuracy=0.996]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20], Train Accuracy: 0.9960, Val Accuracy: 0.9812, Loss: 0.0139\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 600/600 [00:09<00:00, 60.30batch/s, loss=0.012, accuracy=0.997]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20], Train Accuracy: 0.9967, Val Accuracy: 0.9813, Loss: 0.0120\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 600/600 [00:09<00:00, 60.75batch/s, loss=0.00834, accuracy=0.998]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20], Train Accuracy: 0.9980, Val Accuracy: 0.9830, Loss: 0.0083\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 600/600 [00:09<00:00, 63.10batch/s, loss=0.00618, accuracy=0.999]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20], Train Accuracy: 0.9985, Val Accuracy: 0.9786, Loss: 0.0062\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 600/600 [00:09<00:00, 62.63batch/s, loss=0.00661, accuracy=0.998]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20], Train Accuracy: 0.9982, Val Accuracy: 0.9791, Loss: 0.0066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 600/600 [00:10<00:00, 57.73batch/s, loss=0.00674, accuracy=0.998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Train Accuracy: 0.9980, Val Accuracy: 0.9796, Loss: 0.0067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 600/600 [00:11<00:00, 52.51batch/s, loss=0.00656, accuracy=0.998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Train Accuracy: 0.9983, Val Accuracy: 0.9822, Loss: 0.0066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 600/600 [00:11<00:00, 52.77batch/s, loss=0.00684, accuracy=0.998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Train Accuracy: 0.9979, Val Accuracy: 0.9797, Loss: 0.0068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 600/600 [00:11<00:00, 51.97batch/s, loss=0.0028, accuracy=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Train Accuracy: 0.9993, Val Accuracy: 0.9828, Loss: 0.0028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 600/600 [00:11<00:00, 52.85batch/s, loss=0.000685, accuracy=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Train Accuracy: 1.0000, Val Accuracy: 0.9839, Loss: 0.0007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 600/600 [00:11<00:00, 53.03batch/s, loss=0.00442, accuracy=0.999]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Train Accuracy: 0.9987, Val Accuracy: 0.9769, Loss: 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 600/600 [00:11<00:00, 53.00batch/s, loss=0.00968, accuracy=0.997]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Train Accuracy: 0.9965, Val Accuracy: 0.9805, Loss: 0.0097\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 28 * 28  # MNIST image size is 28x28\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input images\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "train_accuracy_history = []\n",
        "val_accuracy_history = []\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    epoch_loss = 0  # Track total loss for the epoch\n",
        "    with tqdm(total=total_step, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({'loss': epoch_loss / (i + 1), 'accuracy': correct / total})\n",
        "            pbar.update()\n",
        "\n",
        "    train_accuracy = correct / total\n",
        "    train_accuracy_history.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        val_accuracy_history.append(val_accuracy)\n",
        "\n",
        "    print ('Epoch [{}/{}], Train Accuracy: {:.4f}, Val Accuracy: {:.4f}, Loss: {:.4f}'\n",
        "           .format(epoch+1, num_epochs, train_accuracy, val_accuracy, epoch_loss / len(train_loader)))\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mlp_model.pth')\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(range(1, num_epochs+1), train_accuracy_history, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), val_accuracy_history, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training History')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFKXdk81uxDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32450c5b-1413-4c1f-9d1b-fda241649c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "pip install Flask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMOoLtH2v-Pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594630fe-ba90-47e3-8d23-20580ebb97a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Define the model again and load the saved model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input images\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "input_size = 28 * 28\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "model.load_state_dict(torch.load('mlp_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to preprocess the uploaded image\n",
        "def preprocess_image(image_bytes):\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert('L')  # Convert image to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize to match the training conditions\n",
        "    ])\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Function to predict the class of a single image\n",
        "def predict_single_image(image):\n",
        "    image = image.to(device)\n",
        "    outputs = model(image)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file provided'}), 400\n",
        "    file = request.files['file']\n",
        "    img_bytes = file.read()\n",
        "    image = preprocess_image(img_bytes)\n",
        "    predicted_label = predict_single_image(image)\n",
        "    return jsonify({'predicted_label': predicted_label})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u4dtWlrwEQ1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}